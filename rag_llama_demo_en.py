from datasets import load_dataset
import torch
import requests
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig

# ==========================================
# 1. é…ç½®è¨­å®š (Configuration)
# ==========================================

# ä½¿ç”¨ Llama 3.1-8B Instructï¼ˆéœ€ Hugging Face æ¬Šé™ï¼‰
MODEL_ID = "meta-llama/Meta-Llama-3.1-8B-Instruct"
# å¦‚æœæ¸¬è©¦è·‘ä¸å‹•ï¼Œä¹Ÿå¯ä»¥æš«æ™‚æ”¹æˆå°ä¸€é»çš„æ¨¡å‹ï¼š
# MODEL_ID = "gpt2"

# ==========================================
# 2. å·¥å…·å‡½æ•¸: Wikipedia RAG
# ==========================================

'''
def search_wikipedia(query: str, num_results: int = 3, lang: str = "en") -> str:
    """
    ç”¨ Wikipedia å®˜æ–¹ API åšç°¡å–® RAGï¼š
    1) å…ˆç”¨ search API æ‰¾åˆ°ç›¸é—œæ¢ç›®
    2) å†ç”¨ pageid æŠ“æ¯å€‹æ¢ç›®çš„æ‘˜è¦ï¼ˆextractï¼‰
    3) å›å‚³æ•´ç†å¥½çš„æ–‡å­—ï¼Œçµ¦ Llama ç•¶ Background Context

    lang å¯ä»¥æ”¹æˆ:
      - "en": è‹±æ–‡ç¶­åŸº
      - "ja": æ—¥æ–‡ç¶­åŸº
      - "zh": ä¸­æ–‡ç¶­åŸº
    """
    try:
        # Step 1: search
        search_url = f"https://{lang}.wikipedia.org/w/api.php"
        search_params = {
            "action": "query",
            "list": "search",
            "srsearch": query,
            "srlimit": num_results,
            "format": "json"
        }
        r = requests.get(search_url, params=search_params, timeout=10)
        data = r.json()

        if "query" not in data or "search" not in data["query"]:
            print("[Wiki] No search results.")
            return ""

        context_text = ""
        for item in data["query"]["search"]:
            title = item.get("title", "")
            pageid = item.get("pageid", None)

            # Step 2: ç”¨ pageid æŠ“æ‘˜è¦
            extract = ""
            if pageid is not None:
                detail_params = {
                    "action": "query",
                    "prop": "extracts",
                    "pageids": pageid,
                    "exintro": True,        # åªè¦é–‹é ­
                    "explaintext": True,    # ç´”æ–‡å­—
                    "format": "json"
                }
                r2 = requests.get(search_url, params=detail_params, timeout=10)
                d2 = r2.json()
                pages = d2.get("query", {}).get("pages", {})
                page = pages.get(str(pageid), {})
                extract = page.get("extract", "")

            context_text += f"- Title: {title}\n  Snippet: {extract[:300]}\n"

        return context_text.strip()

    except Exception as e:
        print(f"[Wiki] Error: {e}")
        return ""

'''
'''
def search_wikipedia(query: str, num_results: int = 3, lang: str = "en") -> str:
    """
    ç”¨ Wikipedia å®˜æ–¹ API åšç°¡å–® RAGï¼š
    1) å…ˆç”¨ search API æ‰¾åˆ°ç›¸é—œæ¢ç›®
    2) å†ç”¨ pageid æŠ“æ¯å€‹æ¢ç›®çš„æ‘˜è¦ï¼ˆextractï¼‰
    3) å›å‚³æ•´ç†å¥½çš„æ–‡å­—ï¼Œçµ¦ Llama ç•¶ Background Context
    """
    print(f"[Wiki] query = {query!r}")

    try:
        # Step 1: search
        search_url = f"https://{lang}.wikipedia.org/w/api.php"
        search_params = {
            "action": "query",
            "list": "search",
            "srsearch": query,
            "srlimit": num_results,
            "format": "json",
        }

        r = requests.get(search_url, params=search_params, timeout=10)
        print(f"[Wiki] HTTP status = {r.status_code}")
        # çœ‹ä¸€ä¸‹å‰é¢å¹¾å€‹å­—ï¼Œç¢ºèªæ˜¯ä¸æ˜¯ JSON
        print(f"[Wiki] raw text (å‰80å­—) = {r.text[:80]!r}")

        data = r.json()

        if "query" not in data or "search" not in data["query"]:
            print("[Wiki] No search results in JSON.")
            return ""

        context_lines = []

        for item in data["query"]["search"]:
            title = item.get("title", "")
            pageid = item.get("pageid")

            extract = ""
            if pageid is not None:
                detail_params = {
                    "action": "query",
                    "prop": "extracts",
                    "pageids": pageid,
                    "exintro": True,        # åªè¦é–‹é ­
                    "explaintext": True,    # ç´”æ–‡å­—
                    "format": "json",
                }
                r2 = requests.get(search_url, params=detail_params, timeout=10)
                d2 = r2.json()
                pages = d2.get("query", {}).get("pages", {})
                page = pages.get(str(pageid), {})
                extract = page.get("extract", "")

            line = f"- Title: {title}\n  Snippet: {extract[:300]}"
            context_lines.append(line)

        context_text = "\n".join(context_lines)
        return context_text

    except Exception as e:
        print(f"[Wiki] Error: {e}")
        # fallbackï¼šä¸è¦è®“æ•´å€‹ pipeline æ›æ‰ï¼Œçµ¦ä¸€æ®µå‡çš„èƒŒæ™¯
        fallback = f"""
- Title: æ¨¡æ“¬èƒŒæ™¯ï¼ˆWiki é€£ç·šå¤±æ•—ï¼‰
  Snippet: åŸæœ¬è¦å¾ Wikipedia æŸ¥è©¢ã€Œ{query[:20]}ã€ï¼Œä½†ç›®å‰ç’°å¢ƒç„¡æ³•æ­£å¸¸å–å¾—çµæœã€‚
        """.strip()
        return fallback
'''

def search_wikipedia(query: str, num_results: int = 3, lang: str = "en") -> str:
    """
    ç”¨ Wikipedia å®˜æ–¹ API åšç°¡å–® RAGï¼š
    1) å…ˆç”¨ search API æ‰¾åˆ°ç›¸é—œæ¢ç›®
    2) å†ç”¨ pageid æŠ“æ¯å€‹æ¢ç›®çš„æ‘˜è¦ï¼ˆextractï¼‰
    3) å›å‚³æ•´ç†å¥½çš„æ–‡å­—ï¼Œçµ¦ Llama ç•¶ Background Context
    """
    print(f"[Wiki] query = {query!r}")

    # å®˜æ–¹å»ºè­°è¦å¸¶ user-agentï¼Œé¿å…è¢«æ“‹
    headers = {
        "User-Agent": "NTU-ADL-FinalProject/0.1",
        "Accept": "application/json",
    }

    try:
        # Step 1: search
        search_url = f"https://{lang}.wikipedia.org/w/api.php"
        search_params = {
            "action": "query",
            "list": "search",
            "srsearch": query,
            "srlimit": num_results,
            "format": "json",
        }

        r = requests.get(search_url, params=search_params, headers=headers, timeout=10)
        print(f"[Wiki] HTTP status = {r.status_code}")
        print(f"[Wiki] raw text (å‰80å­—) = {r.text[:80]!r}")

        # å¦‚æœä¸æ˜¯ 200ï¼Œæˆ–çœ‹èµ·ä¾†ä¸åƒ JSONï¼Œå°±ç›´æ¥ fallback
        if r.status_code != 200 or not r.text.strip().startswith("{"):
            raise RuntimeError(f"Unexpected response from Wikipedia: status={r.status_code}")

        data = r.json()

        if "query" not in data or "search" not in data["query"]:
            print("[Wiki] No search results in JSON.")
            return ""

        context_lines = []

        for item in data["query"]["search"]:
            title = item.get("title", "")
            pageid = item.get("pageid")

            extract = ""
            if pageid is not None:
                detail_params = {
                    "action": "query",
                    "prop": "extracts",
                    "pageids": pageid,
                    "exintro": True,        # åªè¦é–‹é ­
                    "explaintext": True,    # ç´”æ–‡å­—
                    "format": "json",
                }
                r2 = requests.get(search_url, params=detail_params, headers=headers, timeout=10)
                d2 = r2.json()
                pages = d2.get("query", {}).get("pages", {})
                page = pages.get(str(pageid), {})
                extract = page.get("extract", "")

            line = f"- Title: {title}\n  Snippet: {extract[:300]}"
            context_lines.append(line)

        context_text = "\n".join(context_lines)
        return context_text

    except Exception as e:
        print(f"[Wiki] Error: {e}")
        # ğŸ” fallbackï¼šä¸è¦è®“æ•´å€‹ pipeline æ›æ‰ï¼Œçµ¦ä¸€æ®µå‡çš„èƒŒæ™¯
        fallback = f"""
- Title: æ¨¡æ“¬èƒŒæ™¯ï¼ˆWiki æœªå–å¾—æ­£å¸¸çµæœï¼‰
  Snippet: åŸæœ¬è¦å¾ Wikipedia æŸ¥è©¢ã€Œ{query[:20]}ã€ï¼Œä½†ç›®å‰ç’°å¢ƒç„¡æ³•æ­£å¸¸å–å¾— JSON å›æ‡‰ã€‚
        """.strip()
        return fallback

# ==========================================
# 3. å·¥å…·é¡åˆ¥: Llama 3.1 ç”Ÿæˆå™¨
# ==========================================

class LlamaEngine:
    def __init__(self, model_id: str):
        print(f"Loading model: {model_id} (4-bit quantization if supported)...")

        # ä½¿ç”¨ 4-bit é‡åŒ–ä»¥ç¯€çœ VRAM
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_quant_type="nf4"
        )

        self.tokenizer = AutoTokenizer.from_pretrained(model_id)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_id,
            quantization_config=bnb_config,
            device_map="auto"
        )

        self.pipe = pipeline(
            "text-generation",
            model=self.model,
            tokenizer=self.tokenizer,
            max_new_tokens=512,
            temperature=0.7,
            top_p=0.9
        )

    def generate_fake_news(self, real_news, context: str) -> str:
        """
        æ ¹æ“šçœŸå¯¦æ–°è + RAG èƒŒæ™¯è³‡è¨Šï¼Œç”¢ç”Ÿã€Œå‡æ–°èç‰ˆæœ¬ã€ã€‚
        real_news: dict, { "title": str, "text": str }
        context: str, ä¾†è‡ª Wikipedia çš„èƒŒæ™¯æ‘˜è¦
        """
        title = real_news["title"]
        content = real_news["text"][:1000]  # é¿å…å¤ªé•·

        system_prompt = (
            "You are a sophisticated writer. Your task is to rewrite a real news story "
            "to introduce believable factual errors or alter key entities (names, locations, events) "
            "while maintaining the journalistic tone. "
            "The goal is to create a piece of 'Fake News' that is plausible enough to fool fact-checkers. "
            "This is only for research and model training, not for real-world publishing."
        )

        user_prompt = f"""
### Background Information (RAG Context from Wikipedia):
{context}

### Original Real News:
Title: {title}
Content: {content}

### Task:
Please rewrite the news above.
1. Use the Background Information to add realistic details but twist the main facts.
2. Keep the style professional, like a news article.
3. Output format:
Title: [New Title]
Body: [New Body]
        """.strip()

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        # å°‡å°è©±æ ¼å¼è½‰æˆæ¨¡å‹çš„ prompt
        prompt = self.tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )

        outputs = self.pipe(prompt)
        generated_text = outputs[0]["generated_text"]

        # å¦‚æœæ˜¯ Llama-3.1 çš„ chat æ¨¡æ¿ï¼Œå¯èƒ½æœƒåŒ…å«ç‰¹æ®Š tokenï¼Œé€™è£¡åšå€‹ç°¡å–®åˆ‡å‰²
        split_tok = "<|start_header_id|>assistant<|end_header_id|>"
        if split_tok in generated_text:
            generated_text = generated_text.split(split_tok)[-1].strip()

        return generated_text

# ==========================================
# 4. ä¸»ç¨‹åº
# ==========================================

def main():
    # --- A. è¼‰å…¥çœŸå¯¦æ–°è Dataset ---
    print("Loading Dataset (CNN/DailyMail, test[0])...")
    dataset = load_dataset("cnn_dailymail", "3.0.0", split="test[:1]")  # åªæ‹¿ 1 ç­†

    # --- B. åˆå§‹åŒ– Llama å¼•æ“ ---
    llama = LlamaEngine(MODEL_ID)

    # --- C. è™•ç†æ¯ä¸€å‰‡æ–°èï¼ˆé€™è£¡åªæœ‰ 1 å‰‡ï¼‰ ---
    for i, news_item in enumerate(dataset):
        print(f"\n{'='*20} Processing News {i+1} {'='*20}")

        # CNN/DailyMail çš„æ¬„ä½æ˜¯ 'article'
        article_text = news_item["article"]
        original_snippet = article_text[:300].replace("\n", " ")
        formatted_news = {
            "title": "Breaking News",  # å¦‚æœæ²’æœ‰ title æ¬„ä½ï¼Œå°±å…ˆçµ¦ä¸€å€‹ placeholder
            "text": article_text
        }

        print(f"\n[Original News Snippet]:\n{original_snippet}")

        # --- D. RAGï¼šç”¨ Wikipedia ç•¶å¤–éƒ¨çŸ¥è­˜ä¾†æº ---
        search_query = formatted_news["text"][:50].replace("\n", " ")
        print(f"\n[RAG] Using Wikipedia with query:\n{search_query}")

        # lang="en" ç”¨è‹±æ–‡ç¶­åŸºï¼›ä¹‹å¾Œå¯ä»¥æ”¹æˆ "ja" / "zh"
        rag_context = search_wikipedia(search_query, num_results=3, lang="en")
        print(f"\n[RAG Context from Wikipedia]:\n{rag_context}")

        # --- E. ç”Ÿæˆå‡æ–°è ---
        fake_news = llama.generate_fake_news(formatted_news, rag_context)

        print(f"\n[Generated Fake News]:\n{fake_news}")
        print("-" * 80)


if __name__ == "__main__":
    main()
